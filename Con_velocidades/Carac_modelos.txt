Modelo 1: m = 500, funciones de activacion = SELU sin dropout despues de cada capa oculta, con Batch Normalization, y LeCun Normal como incialización.
	n_hidden = 6
	n_neurons = 384
	lr = 0.00611453
	optimizador = descenso del gradiente
	
Modelo 2: m = 1000, funciones de activacion = SELU sin dropout despues de cada capa oculta, con Batch Normalization, y LeCun Normal como incialización.
	n_hidden = 7
	n_neurons = 407
	lr = 0.0011182
	optimizador = descenso del gradiente
	
Modelo 3: m = 1000, funciones de activacion = SELU sin dropout despues de cada capa oculta, con BAtch Normalization, y LeCun Normal como incialización.
	n_hidden = 7
	n_neurons = 400
	lr = 0.001791702
	optimizador = descenso del gradiente
	
Modelo 4: m = 2000, funciones de activacion = SELU sin dropout despues de cada capa oculta, con BAtch Normalization, y LeCun Normal como incialización.
	n_hidden = 8
	n_neurons = 400 de 1 a 7 y la 8 500
	lr = 0.001791702
	optimizador = descenso del gradiente
	
Modelo 5: m = 3500, funciones de activacion = SELU sin dropout despues de cada capa oculta, con BAtch Normalization, y LeCun Normal como incialización.
	n_hidden = 9
	n_neurons = 400 de 1 a 7 y 500 de 8 a 9
	lr = 0.001791702
	optimizador = descenso del gradiente


